Спасибо за подробные уточнения! Я проведу исследование по модификации `ARWallPresenter` и шейдеров в Unity для достижения фотореалистичного наложения цвета на стены, сохраняя освещение и текстуру, как в Dulux Visualizer. Также рассмотрю плюсы и минусы Shader Graph и HLSL, с акцентом на производительность на iOS и Android. Скоро вернусь с результатами.


# Photorealistic Wall Recoloring in AR: Blending Techniques and Shader Strategy

## Preserving Wall Texture and Realism with Blending

To make a digitally recolored wall look like **real paint** rather than a flat overlay, we must preserve the wall’s **texture details and lighting** in the camera feed. The key is to change the wall’s hue while keeping its original luminance (brightness) and fine detail:

* **Luminance-Based Recoloring:** A common approach is to convert the original camera color to a space separating luminance and chrominance. For example, using a Y’CbCr or HSL color model, take the original pixel’s luminance *Y* (which encodes lighting/shadow and texture) and combine it with the *hue* and *saturation* of the desired paint color. This essentially mimics Photoshop’s “Color” blend mode – *“the result color has the luminance of the base (original) and the hue/saturation of the blend (paint color), preserving the gray levels of the image”*. In practice, one can compute a grayscale value of the wall pixel and use it to modulate the new color.
* **Implementing Luminance Preservation:** Use Rec.709 luminance weights (approximately **0.2126R + 0.7152G + 0.0722B**) to get the wall pixel’s brightness. Let this be `L_orig`. For a chosen paint color (in linear RGB) with components `(R_p, G_p, B_p)`, you can form the output as `OutputRGB = (R_p, G_p, B_p) * L_orig`. This keeps shading, cracks, and surface variation from the original wall (encoded in `L_orig`) while applying the new hue. Essentially, darker parts of the wall remain darker even after recoloring, and highlights/shadows stay intact, yielding a natural look.
* **Avoiding “Tinted Film” Look:** By completely replacing the wall’s chroma and not just overlaying semi-transparent color, the original wall color is fully replaced by the new color. Only the intensity (and fine details) remain from the original. This prevents the result from looking like a sheer tint. For example, if the wall had subtle cracks or stucco texture causing small brightness changes, those will still show through the new paint as they would on a real painted wall. However, broad color patterns (like a painted mural) on the original wall would be covered by the new color (as real paint would cover them), aside from any relief differences captured in luminance.

> **Technique Note:** The simplest implementation is to compute a **grayscale texture of the wall** and multiply it by a solid paint color in the shader. This is equivalent to the “Color” blend mode mentioned above and ensures the **original wall’s light/shadow** is preserved. One caution is that if the user selects a very dark or very bright color compared to the original wall, the perceived brightness might differ from reality. (For instance, painting a white wall a deep navy blue would realistically darken its appearance under the same lighting. Our method will produce a dark navy since `L_orig` from the white wall is high but the navy color’s RGB is low.) In most cases, this direct modulation works well; for extra accuracy, you could introduce an adjustable factor to account for differences in paint reflectance if needed.

## Real-Time Lighting Adaptation using AR Light Estimation

Realistic appearance also requires responding to **ambient lighting** so the new color looks correctly lit in the AR scene. Unity’s AR Foundation provides light estimation data that we can leverage:

* **Ambient Brightness:** ARFoundation supplies an `averageBrightness` (or `averageIntensityInLumens`) which indicates the overall scene light level. We should ensure our recoloring shader uses the actual camera pixel brightness (as described above) so it inherently reflects ambient light. In other words, if the room lights dim, the camera feed wall pixels get darker and our output color via the grayscale will also get darker, matching the environment. If needed, we could also globally multiply the output by `averageBrightness` to scale brightness to physical units, but since we preserve the camera’s luminance, this is usually already handled.
* **Light Color (Color Temperature):** AR devices can estimate the scene’s color tint via `colorCorrection` or color temperature data. AR Foundation’s `ARLightEstimationData.colorCorrection` provides RGB factors to match virtual object shading to the real light. Applying this to the paint color ensures the recolored wall is tinted by the light in the room. For example, under warm incandescent lighting, a pure white paint would actually look yellowish. We can multiply our output color by the `colorCorrection` color to mimic this effect. This maintains consistency between the recolored wall and other real objects: *the wall’s new color will take on the scene’s lighting tint and intensity, preventing it from looking “stuck on”*.
* **Main Light Direction (if available):** On advanced devices (e.g. LiDAR-equipped iPhones), ARKit provides a main light direction and intensity. While our approach is image-based, if we have the **wall’s 3D plane** (from AR plane detection or depth) and its normal, we could use the main light direction to apply a slight shading or normal map for the wall material. For instance, one could composite a *virtual diffuse lighting* on the recolored pixels: `OutputRGB = (R_p, G_p, B_p) * L_orig * max(0, N·L)` for a directional light L. However, this is an optional enhancement – the camera feed already contains real shading. For most cases, using the camera luminance plus color correction gives a convincing result.

## Shader Graph vs. HLSL – Choosing an Implementation Path

Both **Unity Shader Graph** and hand-written **HLSL shaders** can achieve this effect, but there are trade-offs in development speed vs. control:

* **Shader Graph Pros:** It is excellent for rapid prototyping and visualization. You can quickly build the blending logic (e.g. sample camera texture, sample mask, compute grayscale, multiply by color) without writing code. The node-based interface provides *immediate visual feedback* when tweaking the blending formula, and it avoids syntax errors. Given that our effect involves relatively standard operations (texture sampling, dot products for luminance, multiplication), Shader Graph can implement them readily. It’s also easier for team members less familiar with HLSL to understand and modify. Using Shader Graph also ensures compatibility with the render pipeline (URP) out-of-the-box – important for AR Foundation, which often uses URP.

* **Shader Graph Cons:** The **flexibility and performance** can be somewhat limited for advanced needs. Shader Graph might introduce minor overhead and lacks some low-level features. For example, if we needed to use a custom memory layout, dynamic loops/branches, or platform-specific optimizations, Shader Graph could be restrictive. In 2025, Shader Graph still doesn’t expose certain features (like stencil buffer operations, some depth interactions, etc.) and very complex math graphs can become unwieldy. For our wall painting shader, these limitations are not severe – it’s a straightforward pixel operation – but it’s worth noting.

* **HLSL Pros:** Writing the shader in HLSL via ShaderLab gives **maximum control and optimization** potential. We can fine-tune arithmetic precision (using `half` where suitable), avoid any redundant calculations that a graph might include, and easily integrate conditional logic or custom lighting models. An HLSL shader could also be extended to do things like multi-pass blending, use the depth texture or custom buffers if we later incorporate geometry or occlusion. If profiling shows the Shader Graph version is a bottleneck, a hand-optimized HLSL shader could be more efficient – though a well-made Shader Graph usually compiles to similar HLSL under the hood.

* **HLSL Cons:** The downside is the **steeper learning curve and longer development** time. Debugging shader code can be tricky, and any issues could slow down development. Since our use-case is relatively narrow, a full custom shader might be overkill unless we hit performance problems. A good compromise can be to **prototype in Shader Graph** (for speed and easy iteration on the blending visuals) and then, if necessary, **convert to HLSL**. Unity even allows embedding HLSL snippets in Shader Graph via Custom Function nodes, which could be useful if, say, we want to write a small function to convert RGB to HSV or YCbCr inside the graph.

**Recommendation:** Start with a Shader Graph implementation of the blending shader for ARWallPresenter. Unity’s Shader Graph supports sampling the camera feed texture (which ARCameraBackground can provide) and the segmentation mask texture, then performing the multiply-blend. This will let you quickly tweak the effect (e.g. adjusting saturation of the paint, or adding a slight mix with original color if needed). Keep an eye on performance; if the graph-shader ends up heavy or if you need a feature outside its scope, plan to port the final shader to HLSL for maximum optimization. In many cases, the graph will suffice, but having the HLSL “upgrade” path ensures you can meet performance goals on lower-end devices.

## Cross-Platform Considerations and Performance

Ensuring the solution works **smoothly on both iOS (ARKit) and Android (ARCore)** is crucial. Fortunately, AR Foundation abstracts most platform differences:

* **ARFoundation Compatibility:** The light estimation features discussed (ambient intensity, color temperature) are supported on both ARKit and ARCore (where available) via the same `ARLightEstimationData` API. The camera feed texture is accessible through AR Foundation’s `ARCameraBackground`. On ARKit, you may get additional data (like directional light estimates), while on ARCore you’ll rely on ambient light and color correction, but our shader can use whatever data is present. Make sure to enable **Light Estimation** in the ARSession configuration so that these values populate. The **segmentation mask** generation (via AsyncSegmentationManager/WallSegmentation) should be tuned per platform if needed (e.g., ensure the model or method runs efficiently on both CPU architectures or use GPU APIs like Metal/Vulkan appropriately).

* **Shader Performance:** Mobile AR rendering is effectively a full-screen shader every frame (for the camera feed) – adding our custom blending will add some overhead, but it must remain lightweight. To optimize:

  * Use **optimized texture sampling** – sample the camera texture and the mask as few times as possible (one each per pixel). If using Shader Graph, connect the texture nodes directly to avoid duplicate samplers, or in HLSL ensure you sample once and reuse the value.
  * Keep calculations per fragment minimal (a couple of dot products and multiplies are fine). Avoid expensive operations like trigonometric functions or loops in the fragment shader.
  * Utilize **half precision** (FP16) for mobile shaders where possible. Colors and luminance can often be calculated in half precision, which can yield performance gains on mobile GPUs.
  * Ensure the shader is **flagged as Unlit** (since we are manually handling lighting via the camera brightness). Relying on the camera feed for lighting means we don’t need Unity’s per-pixel lights or additional lighting calculations.
  * If the wall segmentation mask has hard edges, consider a slight **feathering or smoothing** at the edges in the shader to make the paint blend transition more natural. This could be done by sampling a slightly blurred version of the mask or using bilinear sampling on a downscaled mask texture. However, use this only if needed, as additional samples could cost performance.

* **Battery and Thermal Impact:** Running segmentation and a custom shader will tax the device. To maintain a smooth experience:

  * Try performing segmentation at a lower resolution if possible, and upscale the mask in the shader with interpolation. Often a quarter-resolution mask can be sufficient when smoothly applied, dramatically reducing CPU/GPU load for segmentation.
  * Use AR Foundation’s features like **XROcclusionSubsystem** carefully – if not using depth or human occlusion, disable those to save processing.
  * Test on mid-range devices (Android mid-tier phones, older iPhones) to ensure the frame rate stays high. Optimize the shader or reduce camera resolution if you notice frame drops.

By keeping the architecture the same (AsyncSegmentationManager → WallSegmentation → ARWallPresenter), we focus improvements in ARWallPresenter’s **shading logic**. The techniques above – **luminance-preserving color blending** and **lighting adaptation via AR data** – will make the recolored wall appear impressively real under varying conditions. The development can be done iteratively with Shader Graph for quick results, and later solidified with HLSL if needed for finesse. The end result will be an AR wall-paint effect that maintains the wall’s texture and responds to room lighting, delivering a high level of photorealism.

**Sources:**

* Adobe Photoshop Blend Mode Definitions (on preserving luminosity for color blending)
* Computer Graphics QA – Luminance Preservation Techniques
* Unity ARFoundation Light Estimation Documentation
* Unity Shader Graph vs. HLSL – Performance and Flexibility Considerations
* StackOverflow – Shader recoloring with luminance (example using Rec.709 weights)
